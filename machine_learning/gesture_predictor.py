import cv2
import mediapipe as mp
import numpy as np
import joblib
import time

class GesturePredictor:
    def __init__(self, model_path="model.pkl", scaler_path="scaler.pkl"):
        # åŠ è½½æ¨¡å‹
        self.model = joblib.load(model_path)
        self.scaler = joblib.load(scaler_path)

        # æ‰‹åŠ¿æ ‡ç­¾åç§°
        self.label_names = ["zero", "one", "two", "three", "four", "five", 
                           "six", "seven", "eight", "nine", "ten"]

        # åˆå§‹åŒ–MediaPipe
        self.mp_hands = mp.solutions.hands
        self.mp_drawing = mp.solutions.drawing_utils

        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œå‡†å¤‡å¼€å§‹è¯†åˆ«...")

    def extract_features(self, hand_landmarks):
        """ä»MediaPipeå…³é”®ç‚¹æå–ç‰¹å¾"""
        landmarks = hand_landmarks.landmark
        features = []
        for lm in landmarks:
            features.extend([lm.x, lm.y])  # åªä½¿ç”¨x,yåæ ‡
        return np.array(features).reshape(1, -1)

    def predict_gesture(self, features):
        """é¢„æµ‹æ‰‹åŠ¿"""
        # æ ‡å‡†åŒ–ç‰¹å¾
        features_scaled = self.scaler.transform(features)

        # é¢„æµ‹
        prediction = self.model.predict(features_scaled)[0]
        probabilities = self.model.predict_proba(features_scaled)[0]

        # è·å–Top-3é¢„æµ‹
        top3_idx = np.argsort(probabilities)[-3:][::-1]
        top3 = [(self.label_names[i], probabilities[i]) for i in top3_idx]

        return int(prediction), top3

    def run_realtime(self):
        """å®æ—¶æ‰‹åŠ¿è¯†åˆ«"""
        cap = cv2.VideoCapture(0)

        with self.mp_hands.Hands(
            min_detection_confidence=0.7,
            min_tracking_confidence=0.5,
            max_num_hands=1
        ) as hands:

            fps_time = 0
            predictions_history = []

            while True:
                # è¯»å–å¸§
                ret, frame = cap.read()
                if not ret:
                    break

                frame = cv2.flip(frame, 1)
                height, width, _ = frame.shape

                # è®¡ç®—FPS
                fps = 1.0 / (time.time() - fps_time) if fps_time > 0 else 0
                fps_time = time.time()

                # è½¬æ¢ä¸ºRGB
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                # æ£€æµ‹æ‰‹åŠ¿
                results = hands.process(rgb_frame)

                # æ˜¾ç¤ºFPS
                cv2.putText(frame, f"FPS: {fps:.1f}", (10, 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

                if results.multi_hand_landmarks:
                    # åªå¤„ç†ç¬¬ä¸€åªæ‰‹
                    hand_landmarks = results.multi_hand_landmarks[0]

                    # ç»˜åˆ¶å…³é”®ç‚¹
                    self.mp_drawing.draw_landmarks(
                        frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS)

                    # æå–ç‰¹å¾å¹¶é¢„æµ‹
                    features = self.extract_features(hand_landmarks)
                    prediction, top3 = self.predict_gesture(features)

                    # æ·»åŠ åˆ°å†å²ï¼ˆç”¨äºå¹³æ»‘ï¼‰
                    predictions_history.append(prediction)
                    if len(predictions_history) > 5:
                        predictions_history.pop(0)

                    # ä½¿ç”¨ä¼—æ•°å¹³æ»‘é¢„æµ‹ç»“æœ
                    smoothed_prediction = max(set(predictions_history), 
                                            key=predictions_history.count)

                    # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
                    cv2.putText(frame, 
                              f"Gesture: {self.label_names[smoothed_prediction]} ({smoothed_prediction})",
                              (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

                    # æ˜¾ç¤ºç½®ä¿¡åº¦
                    cv2.putText(frame, 
                              f"Confidence: {top3[0][1]:.2f}",
                              (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)

                    # æ˜¾ç¤ºTop-3é¢„æµ‹
                    for i, (name, prob) in enumerate(top3[:3]):
                        y_pos = 150 + i * 30
                        color = (0, 200, 0) if i == 0 else (200, 200, 0)
                        cv2.putText(frame, 
                                  f"{name}: {prob:.3f}",
                                  (10, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 1)
                else:
                    cv2.putText(frame, "No hand detected", (10, 70),
                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

                # æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
                cv2.putText(frame, "Press 'q' to quit", (width-200, 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)

                # æ˜¾ç¤ºçª—å£
                cv2.imshow("Gesture Recognition 1-10", frame)

                # æŒ‰qé€€å‡º
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break

        cap.release()
        cv2.destroyAllWindows()

if __name__ == "__main__":
    print("ğŸ® æ‰‹åŠ¿1-10å®æ—¶è¯†åˆ«ç³»ç»Ÿ")
    print("è¯·ç¡®ä¿å·²ç»è¿è¡Œ train_model.py è®­ç»ƒäº†æ¨¡å‹")

    predictor = GesturePredictor()
    predictor.run_realtime()